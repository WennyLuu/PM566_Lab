---
title: "PM566 Lab07"
author: "Wenjia Lu"
format:
  html:
    embed-resources: true
---

```{r}
## Set packages and download data ##

library(microbenchmark)
library(parallel)

```

```{r}
## Problem 1: Vectorization ##

# Write a faster version of each function and show that (1) the outputs are the same as the slow version, (2) your version is faster.

```

```{r}
# 1.This function generates an n x k dataset with all its entries drawn from a Poission distribution with mean lambda.
fun1 <- function(n = 100, k = 4, lambda = 4) {   
  x <- NULL
  
  for (i in 1:n){     
    x <- rbind(x, rpois(k, lambda))  
    }
  return(x) 
}

# Show that fun1alt generates a matrix with the same dimensions as fun1 and that the values inside the two matrices follow similar distributions.
fun1alt <- function(n = 100, k = 4, lambda = 4) {   
  x <- matrix( rpois(n*k, lambda) , ncol=4)  
  return(x) 
}

# Check the speed of the two functions
# Benchmarking 
microbenchmark::microbenchmark(   
  fun1(),   
  fun1alt() 
)

```

```{r}
# 2.This function finds the maximum value of each column of a matrix (hint: check out the max.col() function).

# Data Generating Process (10 x 10,000 matrix) 
set.seed(1234) 
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value 
fun2 <- function(x) {   
  apply(x, 2, max) 
  return(x)
}

# Show that both functions return the same output for a given input matrix, x. 
fun2alt <- function(x) {   
  idx= max.col(t(x))
  x[cbind(idx, 1:4)]
}

#Check the speed of the two functions.
microbenchmark::microbenchmark(   
  fun2(x),   
  fun2alt(x) 
)

```

```{r}
## Problem 2: Parallelization ##

# The main assumption is that we can approximate the results of many repeated experiments by resampling observations from our original dataset, which reflects the population.

```

```{r}
# 1.Edit this function to parallelize the lapply loop. Use the number given by the ncpus argument, so that we can test it with different numbers of cores later.

my_boot <- function(dat, stat, R, ncpus = 1L) {
  # Getting the random indices   
  n <- nrow(dat)   
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
  
  # THIS FUNCTION NEEDS TO BE PARALELLIZED, EDIT THIS CODE: 
  # STEP 1: GOES HERE
  cl <- makePSOCKcluster(4)    
  # STEP 2: GOES HERE
  clusterSetRNGStream(cl, 123)
  clusterExport(cl, c("stat", "dat","idx"), envir = environment())
  # STEP 3: THIS FUNCTION NEEDS TO BE REPLACES WITH parLapply
  ans <- lapply(seq_len(R), function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  })

  # Converting the list into a matrix   
  ans <- do.call(rbind, ans)
  
  return(ans) 
}

```

```{r}
# 2.Once you have a version of the my_boot() function that runs on multiple cores, check that it provides accurate results by comparing it to a parametric model:
# Bootstrap of an OLS 
my_stat <- function(d) coef(lm(y ~ x, data=d))

# DATA SIM 
set.seed(1) 
n <- 500; 
R <- 1e4
x <- cbind(rnorm(n)); y <- x*5 + rnorm(n)

# Checking if we get something similar as lm 
ans0 <- confint(lm(y~x)) 
ans1 <- my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 2L)

t(apply(ans1, 2, quantile, c(.025,.975))) 

ans0 

```

```{r}
# 3.Check whether your version actually goes faster when it’s run on multiple cores (since this might take a little while to run, we’ll use system.time and just run each version once, rather than microbenchmark, which would run each version 100 times, by default):

system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 1L)) 
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 2L))

```
